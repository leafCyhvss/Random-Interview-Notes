
## kafka
### 怎么分消息到不同partition

如果消息有键（Key），默认分区器会根据键的散列值对分区数量进行取模来选择分区。
如果消息没有键，Kafka会以轮询的方式将消息均匀地分配给所有分区。



## 其他人的面试内容
![[Pasted image 20231107184757.png]]
## Java
### 使用的Java版本：
java 8 / 11

没有咋用上java11的新特性，主要还是java 8的 stream / lambda;

java 8 有老module, java 11有性能优化

### Stream
```java

        int[] array = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10};

        int[] evenNumbers = Arrays.stream(array)
                .filter(n -> n % 2 == 0)
                .toArray();
		Map<Integer, Long> elementCounts = Arrays.stream(array)
		.boxed()
		.collect(Collectors.groupingBy(
		
		Function.identity(), 
		Collectors.counting()
    ));



        List<Integer> array2 = Arrays.asList(1,2,3,4,5,6,7,8,9,10);
        List<Integer> array2Even = array2.stream()
                .filter((num) -> num % 2 == 0)
                .collect(Collectors.toList());




		List<Integer> list = Arrays.asList(1,1,9,3,5,3,1,9,9,10);
		Map<Integer, Long> elementCounts = list.stream()
                        .collect(Collectors.groupingBy(
                        e -> e, Collectors.counting()));

        System.out.println(elementCounts);
```


### Optional



## Map
### 种类

1. HashMap
2. LinkedHashMap
3. TreeMap
4. ConcurrentHashMap
5. WeakHashMap: 其键是弱键，当没有其他引用指向键对象时，这些键可以被垃圾收集器回收。这种Map非常适合缓存应用，其中键可能被外部引用所抛弃。
6. EnumMap:是专门为枚举键设计的一种高效率的Map实现。内部以数组形式实现，因此其效率非常高。键必须是单个枚举类型的枚举值，这限制了它的通用性，但提供了速度上的优势。


### TreeMap

底层实现：红黑树，树的每个节点记录一个键值对
自平衡的二叉搜索树
### LinkedHashMap
#### 底层实现
Hashmap is an array of LinkedList. And LinkedHashMap is based on Hashmap, while **each bucket in LinkedHashMap have 2 additional pointers**, pointing to the former node and latter node. It becomes double linked list, which maintains an order of insertion.
(默认是插入顺序)

维护访问顺序：
设置LinkedHashMap的构造函数中的accessOrder参数为true可以实现LRU。 每次get()或put()操作时，将被访问的节点移到内部双向链表的末端。

由于其内部的链表，LinkedHashMap迭代的性能与其大小成正比，与容量无关。

#### 与TreeMap的区别
1. 底层实现不一样
2. 维护的顺序不一样： LinkedHashMap是插入顺序或者访问顺序。TreeMap是按照**order of the keys   或者 指定的 Comparator**  来排序的
3. 性能差距：下面的表格

#### 时间复杂度：
下面是一个表格，列出了`HashMap`，`LinkedHashMap`和`TreeMap`在插入，删除，访问（搜索）和迭代方面的时间复杂度。

| 操作/数据结构 | HashMap | LinkedHashMap | TreeMap |
| -------------- | ------- | ------------- | ------- |
| 插入          | O(1)    | O(1)          | O(log n) |
| 删除          | O(1)    | O(1)          | O(log n) |
| 访问          | O(1)    | O(1)          | O(log n) |
| 迭代          | O(n)    | O(n)          | O(n)     |

迭代性能的差异在于：

- 对于`HashMap`，迭代时间复杂度是O(n)但是可能会受到桶的数量的影响，特别是当许多桶是空的时候。
- `LinkedHashMap`迭代的时间复杂度也是O(n)，但是由于它维护了一个双向链表，它的迭代通常比`HashMap`更快，因为它按照链表顺序直接迭代每个元素，无需关心哈希桶的结构。
- 对于`TreeMap`，迭代时间复杂度也是O(n)，迭代按照键的排序顺序进行，这通常要求遍历整个树结构。

### Concurrent Hashmap

#### 底层原理（Java8以后）
数据结构
桶（Bins）：内部数组结构，与HashMap类似，不过每个桶的头部是一个volatile变量，以确保可见性。
链表：桶中的元素以链表形式存储，链表中的节点用volatile变量存储键和值，确保线程间的可见性。
红黑树：当链表变得太长时，为了提高效率，链表会转换为红黑树，和HashMap类似。

#### 怎么实现thread safe
**锁分离技术**（**Lock Stripping**） （Java8 以后）（java7及以前是分段锁): **每个节点或一组节点提供锁**  1)  对于**无冲突**的简单更新如**值更新**：采用**compare-and-swap**的原子操作。CAS是一种轻量级的锁定机制，可以看作是乐观锁，它检查值是否发生变化来决定是否更新值，不需要使用传统的锁。2）对于那些**可能产生冲突**的更新操作，例如**插入、删除节点**，或者在链表长度超过一定阈值时将链表转换为红黑树，使用**synchronized关键字来加锁**。这里的锁定是**针对特定节点**的，不是针对整个Map的，从而提高了并发性。

**多个读取操作并发进行**： 读取时不需要加锁。由于节点中的数据是用volatile类型声明的，这保证了即使在有写操作发生的时候，读取操作也可以获取到最新的、一致的数据状态。

**Volatile Bins:** 每个桶的头节点都是volatile的，这确保了对桶的写入立即对其他线程可见


### Concurrent HashMap 和 用synchronized锁定的map性能区别：

ConcurrentHashMap 和使用synchronized关键字锁定的普通Map（如通过Collections.synchronizedMap(new HashMap<>())创建的同步Map）在多线程环境下都提供线程安全

**more fine-grained locking**
#### ConcurrentHashMap
1. **锁的粒度**：`ConcurrentHashMap`在Java 7及以前版本中使用一种分段锁（segmentation）的机制。Java 8以后使用lock stripping，基于每个bucket的加锁机制。它允许多个线程同时但安全地更新不同（bucket）的数据。这意味着多个线程可以同时读取和写入`Map`，而不会彼此阻塞，只要它们访问的是不同的bucket。

2. **性能**：由于其细粒度的锁定策略，`ConcurrentHashMap`在高并发环境中通常比`synchronizedMap`提供更高的吞吐量。特别是在读操作远多于写操作的场景中，`ConcurrentHashMap`的性能优势更加明显。

3. **迭代操作**：`ConcurrentHashMap`的迭代器提供了弱一致性（weakly consistent）遍历，而不是快速失败（fail-fast）。这意味着迭代器可以容忍并发修改，反映出迭代开始时或迭代过程中的`Map`状态。

#### SynchronizedMap
1. **锁的粒度**：通过`Collections.synchronizedMap`包装的`Map`（如`HashMap`）在每个操作上都对整个`Map`进行锁定。这意味着任何时刻只有一个线程能够修改`Map`，其他线程必须等待，直到锁被释放。

2. **性能**：因为在任何时候只能有一个线程访问`Map`，所以在高并发场景下，`synchronizedMap`的性能通常低于`ConcurrentHashMap`。尤其在需要频繁读写的应用中，性能差异更为明显。

3. **迭代操作**：`synchronizedMap`的迭代器是快速失败的。如果在迭代过程中`Map`结构发生变化（除了通过迭代器自身的`remove`方法），迭代器将抛出`ConcurrentModificationException`。

### 总结
- 在高并发场景下，特别是当多线程同时读写`Map`时，`ConcurrentHashMap`通常是更好的选择，因为它支持更高级别的并发性。
- 对于线程安全的操作而言，如果不是面对高度并发的环境，可以选择使用`Collections.synchronizedMap`。
- 选择合适的实现应基于具体场景的性能需求和并发级别。

### Map - key - null


| Map 实现          | 允许 null key |
| ----------------- | -------------- |
| HashMap           | 是             |
| LinkedHashMap     | 是             |
| TreeMap           | No             |
| HashTable         | No             |
| ConcurrentHashMap | No             |

ConcurrentHashMap存入key为null时会抛出NPE

```java
public static void main(String[] args) {
        ConcurrentHashMap<Integer, Integer> conMap = new ConcurrentHashMap<>();
//        conMap.put(null, 2);  // >> NPE

        Map<Integer, Integer> linkedMap = new LinkedHashMap<>();
        linkedMap.put(null, 1); // 没问题

        Map<Integer, Integer> treeMap = new TreeMap<>();
//        treeMap.put(null, 1); // NPE

        Map<Integer, Integer> map = new HashMap<>();
        map.put(null,1); // 没问题

        Map<Integer, Integer> table = new Hashtable<>();
//        table.put(null, 1); // NPE
    }
```
## System
### 怎么scale up system

Add more instance of service application
data sharding of db
Dynamic Load Balancing
Message Queuing
upgrade hardware




## 用splunk做什么？怎么用splunk

分析日志，转变成searchable data以供查找


==logstash收集，发到splunk，== 不需要，splunk就能收集log

在splunk中：
配置datasource 
SPL查询log中的error fail  keyword    Splunk Processing Language（打开8080port网页查找）`error OR fail OR "critical error" OR "service unavailable"`


创建图像 显示在dashboard
set alert.


API： QPS / response time
Cpu usage


同时Splunk Universal Forwarder可以收集硬件性能数据，比如cpu占用率

### 为什么用两个数据库
1. 一些customer(company), product 信息是有strict schema, 和一些约束，like foreign keys. we do need transactions when dealing with order creations. And strong data consistency when dealing with payment.
2. 对于合同信息、历史报价记录是灵活的、semi-structured data, like some nested json. MongoDB is suitable for storing such dynamic data in document models. 
3. MongoDB it is highly available when the system is at peek times because it can automatically conduct data sharding which is to scale out  the database.
### 哪里用mongodb

报价历史：数据模式多变
合同数据：包含多层嵌套数据信息 multiple levels of nested data
交易和合同历史：每个订单的交易记录可以作为单独的文档存储，便于分析和检索。

### 为什么用MongoDB而不是cassandra

monodb中数据是文档模型，也就是嵌套的json，可以很灵活地动态地改变结构；自然地表示复杂的层次结构；支持相对复杂query；MongoDB支持自动分片，易于水平扩展，这对于数据量快速增长的订单合同系统来说很重要。

cassandra的好处是快速写入。但是我们用了mq来提供缓冲buffer zone，避免峰值时出问题，所以不太需要。同时还有很多灵活数据模式存入。

### 怎么用redis的
#### 作为MySQL的缓存使用时：
通常可以使用 **Strings** 和 **Hashes** 数据类型。

- **Strings** 可以用于存储查询结果的字符串表示，例如，一个查询返回的JSON字符串。
- **Hashes** 可以用来存储对象形式的数据，如表行的数据。其中，Hash 的键对应数据库表中的列名，Hash 的值对应列的值。

#### 作为存储实时报价信息以供查询的功能使用时：

**Strings** 可以用于存储单个报价信息。如果实时报价不需要按某种顺序检索，或者每个产品的报价只存最新值，使用字符串就足够了
对于每种商品，可以使用一个唯一的键来存储其价格。
例如，商品ID或SKU_id可以作为键，价格作为值。当价格更新时，只需简单地覆盖旧值即可。
Stock keeping unit 库存id

### Oracle和mysql区别

oracle 性能优化功能，如分区、物化视图和高级索引技术。
分析函数、复杂的事务控制
提供全面的事务支持，包括对保存点、回滚、可重复读和读取一致性的支持。
特别适合那些需要非常高可靠性的关键业务应用程序。
提供广泛的安全特性，包括细粒度的访问控制、强制访问控制（如Virtual Private Database）和透明数据加密等。

## Rest
### 200 和 201
get 成功获取(request successful) 200
post  成功创建（resource created) 201；


put: 
200 OK 
或者 204 No Content，客户端不返回anything


失败：
400 bad request
401 Unauthorized（如果需要认证）
403 Forbidden（如果服务器拒绝请
500 internal error
### post put 区别
Post is used to **submit data to be processed** by the resource. It is typically used for **creating new resources** or submitting updates to existing resources. 
While PUT is used to **update an entire existing resource** at the specified URI. The entire representation of the resource is sent in the request, and the server replaces the resource at that URI with the new representation.

Follow up: 
If you make **multiple PUT requests** to update a resource, the end **result will always be the same** as if you made a single PUT request. If you make multiple POST requests to create a resource, it may result in the creation of multiple resources.

**强行使用put创建资源**：
**如果服务器端的实现支持使用 PUT 方法来创建新资源**，并且客户端知道它想要创建资源的URI，那么它**可以**使用 PUT 请求来创建资源。在这种情况下，如果指定的URI不存在，服务器就会创建一个新的资源。

无论 PUT 请求被执行多少次，结果应该保持一致。这意味着如果 PUT 用于创建资源，那么多次执行相同的 PUT 请求应该不会创建多个资源，它只会创建或更新一次。
$idempotent /aɪˈdempətənt/$



## CICD
### 定义
**Continuous integration**  automated building and testing on every new commit.

**Continuous deployment** automation of building, testing, and deploying; push new code through the entire development pipeline to production

### 自己涉及到的流程
My code **PR**    (commit => pull request)
triggers pipelines to **compile and run test auto 
and sonarQube for style checking（不要主动说）**
**peer / code review(left comments)（可以不说）
merge to dev master**

### 整个
上面merge triggers another pipeline： 整个cicd流程：
DEV
Test：QA执行更详尽**more detailed**的测试，包括功能测试、集成测试
Stage: 功能测试（Functional Tests）, load test等(**high level test**)
Prod: **A/B测试或金丝雀发布（Canary Releases）**


## Spring
### 如何创建一个spring project

使用 Spring Initializer (spring 官网 提供的)
配置metadata 比如项目名 版本
导入依赖， 比如Spring Web、Spring Data JPA
生成新项目


### Spring boot版本
2.7

### 如何管理依赖关系：
注意问面试官，是问spring的话回答ioc 和di
问项目的话回答 maven



### @configuration
将一个类标记为 Spring 应用程序上下文中的配置类。这意味着被 @Configuration 注解标记的类可以包含定义 Spring Bean 的方法，这些方法将在 Spring 容器中注册为 Bean 定义。

第三方包，比如swagger； logback; 
Spring: Task Scheduler(Spring task); cache manager



rest template: 配置使用哪个request factory; 配置超时时间
spring security: passwordEncoder()；定义哪些 URL 和端点需要身份验证，以及哪些角色或权限可以访问它们。


**两个作用:**
1. （导入）配置一些库的参数，比如spring security; datasource; 导入xml数据
2. 配置第三方库，返回bean是的其可以被spring管理

先说配置第三方，返回bean




第三方
object mapper
model mapper
rest template
encoder password



## Test

Junit 4 是platform
### @mock @injectmock的区别

@Mock 用于创建一个模拟对象，通常是一个被测试对象依赖的对象，

@InjectMocks 用于创建被测试对象，并尝试将标记为 @Mock 的模拟对象自动注入到被测试对象中的相应字段。

比如今天要测试一个service, 它需要repo类作为依赖，@Mock用在repo上，@injectMock用在service上

还缺一个示例代码

AssertEquals 
AssertNotNUll


```java
@ExtendWith(MockitoExtension.class)
class PostServiceImplTest {

    @Mock
    private PostRepository postRepositoryMock;
    @InjectMocks
    private PostServiceImpl postService;
	
	private PostDto postDto;
    private Post post;
    
    @BeforeAll
    static void beforeAll() {
        logger.info("START test");
    }

    @BeforeEach
    void setUp() {
        logger.info("set up Post for each test");

        this.post = new Post(1L, "xiao ruishi", "wanqu", "wanqu xiao ruishi",
                LocalDateTime.now(), LocalDateTime.now());
        ModelMapper modelMapper = new ModelMapper();
        this.postDto = modelMapper.map(this.post, PostDto.class);
    }

    @Test
    public void testCreatePost() {
        // define the behaviors
        Mockito.when(postRepositoryMock.save(ArgumentMatchers.any(Post.class)))
                .thenReturn(post);

        // execute
        PostDto postResponse = postService.createPost(postDto);

        // assertions
        Assertions.assertNotNull(postResponse);
        Assertions.assertEquals(postDto.getTitle(), postResponse.getTitle());
        Assertions.assertEquals(postDto.getDescription(), postResponse.getDescription());
        Assertions.assertEquals(postDto.getContent(), postResponse.getContent());
    }
}
```


### @spy
创建部分模拟对象：@Spy 用于创建一个部分模拟对象，它会保留被测试对象的实际实现，但可以模拟其中的一些方法。

**经典用途**： 一些utility class，部分功能可以直接用；repo一般是用mock的，不要强行说get data 用真实，修改数据库用mock

部分方法模拟：你可以使用 Mockito 提供的方法，如 when()，来模拟部分方法的行为，而不影响对象的其他方法。
```java
public class SpyExampleTest {

    @Spy
    private MyService myService;

    @Test
    public void testSpy() {
        // 使用 @Spy 创建部分模拟对象，其中一部分方法会被保留
        // 其他方法会执行真实的实现
        when(myService.someMethod()).thenReturn("Mocked Value");
        // 调用实际的方法
        String result = myService.someMethod();
        // 验证模拟方法是否按预期执行
        verify(myService).someMethod();
        // 验证返回值是否是模拟的值
        assertEquals("Mocked Value", result);
        // 实际方法的行为不受影响
        assertEquals("Real Implementation", myService.anotherMethod());
    }
}
```
MyService 类的 **someMethod() 方法被模拟**，而 **anotherMethod() 方法执行其真实的实现**。使用 @Spy 可以在测试中部分模拟对象，以验证其部分行为，同时保留其他方法的真实行为。

### 你做过什么test

unit test 
test the functionality of individual components 


integration test
test the **combined behavior of two or more components** of an application to ensure they **work together as expected**
call 其他api验证不同模块直接是否一起工作

Regression test
@ExtendWith(MockitoExtension.class)
spring boot test runner

@SpringBootTest
@WebMVCTest


### How do you test your API? 
1. (postman, swagger)
2.   integration test


### Regression test

**集成测试**着重于**多个模块或组件如何协同工作**的问题，而**回归测试**则关注在软件开发过程中的更改**是否影响了现有功能的稳定性和正确性**。

1. **集成测试（Integration Test）**：
   - **目的**：集成测试主要是为了验证不同模块或组件在结合后的整体功能和性能。在软件开发过程中，单独的模块可能会按预期工作，但当它们与其他模块集成时，可能会出现问题。集成测试就是为了发现这些问题。
   - **过程**：通常在单元测试之后进行。单元测试关注单个模块或组件的功能，而集成测试则关注这些模块或组件如何协同工作。
   - **方法**：可以采用多种方法进行集成测试，例如逐步集成（逐个添加模块进行测试），大爆炸（同时整合所有模块进行测试），或是使用存根和驱动程序来模拟与其他模块的交互。

2. **回归测试（Regression Test）**：
   - **目的**：回归测试的目的是确保新的代码更改没有破坏现有的功能。它用于验证在软件开发过程中进行的修改，不会导致已有功能的退化或错误。
   - **过程**：在软件开发的任何阶段，只要有代码更改，就可以进行回归测试。这包括新功能的添加、错误修复、性能改进等。
   - **方法**：回归测试通常需要重复之前已经执行过的测试用例。为了提高效率，可以采用自动化测试来执行大量的测试用例。


## BQ

### 为什么要出来找新工作


last project ended because the new system is built and running smoothly now and there is a budget cutting.


### 反问问题

team structure
what ability would you expect from candidates?

could you tell me what a typically day of you would be like ?
The following steps and topic?




## K8S
当然可以，下面是对这些问题的中文回答：

### 如果你在面对部署问题时，你会从哪里开始查找？

首先，我会从**最近一次的变更**开始排查，这可能是代码、配置或依赖库的更新。然后，我会检查持续集成/持续部署（**CI/CD）流程中的输出**，包括部署脚本的执行情况和部署后的健康检查结果。同时，也需要检查**服务配置和环境变量是否在生产环境中被正确设置。**

### 你会查看哪些日志？

我会查看以下日志：
- **应用程序日志**，了解应用内部状态和错误信息。
- 服务器或**容器平台的系统日志**，了解运行环境中可能出现的问题。
- **CI/CD管道中的构建和部署日志**，确定部署流程中的任何失败或警告。
- Web服务器或应用服务器的日志，查看请求处理和响应情况。
- 数据库日志，如果应用依赖数据库服务。
- **检查Monitoring** 

### 如果本地工作正常，但在Pod中失败了，你会如何继续调试？

首先，我会**确认Pod内的环境配置是否与本地环境一致**，包括环境变量和配置文件。然后，我会在Pod内执行一些基本的**网络诊断**，如ping和telnet，以确保Pod可以访问它所依赖的外部服务。

如果以上都没有问题，我会尝试登录到Pod中（例如使用`kubectl exec`），直接**检查pod运行状态和详细日志**。另外，可以尝试在Pod内运行一些测试命令，比如运行应用的健康检查端点。

### 在什么情况下，会增加Pod的数量？

Pod的数量通常会根据以下情况增加：
- **负载增加**：当应用面临更高的流量或负载时，自动伸缩机制（如Kubernetes中的Horizontal Pod Autoscaler）会触发新的Pod实例的创建。
- **高可用性需求**：为了提高应用的可用性，可以部署多个Pod副本，以确保在某个Pod失败时，其他Pod可以继续提供服务。
- **资源利用率**：当现有的Pod资源利用率接近或超过设定阈值时，会增加Pod数量以分散负载。
- **服务扩展计划**：根据应用的扩展策略或业务增长计划，可能会手动或自动增加Pod数量。

在任何情况下，增加Pod的数量都应该基于对现有性能指标和业务需求的细致分析。


可预见的：黑五来了，增加服务
不可预见的：monitoring 发现cpu占用率高，traffic变大，send alert to us
autoscaling : 比如cpu利用率增加到多少